# 1 Multilinear Algebra for Analyzing Data with Multiple Linkages

## 1.内容概述

​    本文主要介绍了CANDECOMP/PARAFAC（CP）张量分解及其常用的一种实现方法ALS（交替最小二乘法，alternating least squares），并以文献数据为例，具体介绍了其实际应用场景，最后简单介绍了相关工作。全文共七节，每一节主要内容如下。

​    第一节为文章内容的概述，说明本文重点为什么是多连接图(Multi-link graphs)和张量（tensor），文章数据如何表示为一张多链接图，多连接图如何表示为张量；同时简单介绍了何为及CP分解及分解后的文献数据的多种分析类型。

​    第二节对张量和CP分解的含义进行了详细的解释。首先介绍了标量（scalar）、向量（vector）、矩阵（matrix）、张量的表示方法和常用运算，然后介绍了CP张量分解的原理，即将张量分解为单秩张量的外积![img](file:///C:/Users/hsc58/AppData/Local/Temp/msohtmlclip1/01/clip_image002.png)，最后具体介绍了CP-ALS算法的原理和实现方法。

​    第三节为文献数据组织方式和预处理方式的介绍，说明了在图中，每个结点代表一篇文章，节点间有五种连接类型，以邻接矩阵的方式，表示为![img](file:///C:/Users/hsc58/AppData/Local/Temp/msohtmlclip1/01/clip_image004.png)的张量，其中每个通道切片（frontal slice），即![img](file:///C:/Users/hsc58/AppData/Local/Temp/msohtmlclip1/01/clip_image006.png)，分别为摘要相似度、标题相似度、作者提供的关键词相似度、作者相似度、引用信息，并给出各类连接具体数值的计算方法和定量测量结果。

​    第四节为将上一步得到数据张量采用CP分解后得到的结果，包括以下几个部分。一是community identification，即对于第![img](file:///C:/Users/hsc58/AppData/Local/Temp/msohtmlclip1/01/clip_image008.png)个factor，在![img](file:///C:/Users/hsc58/AppData/Local/Temp/msohtmlclip1/01/clip_image010.png)中具有高连接评分（link score）则*A*中高评分的节点与*B*中高评分节点相关联；二是运用分量矩阵的组合来发现潜在相似度；三是利用质心及其组合来进行分析，四是关于同名作者是否为同一人的区分方法，最后是使用有监督的方法利用特征向量来预测文章可能出现在哪一个期刊上。

​    第五节主要对出版数据（publication data）的分析、数据挖掘在高阶数据中的分析的相关工作进行介绍。第六节为文章总结和后续工作，第七节为致谢。

## 2.个人感悟与想法

​    这是一篇十分详尽的文章，介绍了CP-ALS分解所需要的知识和原理，并完整地给出了一个数据分析的例子。在基础知识部分，通过阅读文章和查阅相关内容，我了解到了许多矩阵有关的运算，通过尝试推导CP的基本原理，发现自身需要进一步提高线性代数相关知识的学习。在数据张量构造的部分，我进一步了解到余弦相似度的运用方法和使用场景，以及启发式确定具体数值的办法。除此之外，本文利用![img](file:///C:/Users/hsc58/AppData/Local/Temp/msohtmlclip1/01/clip_image012.png)三元组来表示在![img](file:///C:/Users/hsc58/AppData/Local/Temp/msohtmlclip1/01/clip_image014.png)关系下![img](file:///C:/Users/hsc58/AppData/Local/Temp/msohtmlclip1/01/clip_image016.png)对![img](file:///C:/Users/hsc58/AppData/Local/Temp/msohtmlclip1/01/clip_image018.png)的关系，最终表示成一个三阶张量，让我对数据的预处理方式有了新的想法。如果本文的问题直接给我，我大概会最终将其化为一个矩阵，将![img](file:///C:/Users/hsc58/AppData/Local/Temp/msohtmlclip1/01/clip_image016.png)对![img](file:///C:/Users/hsc58/AppData/Local/Temp/msohtmlclip1/01/clip_image018.png)的关系利用特征降维的方法压缩为一个值，最终在这个矩阵上进行处理和分析。同时，本题的张量实际上为多个邻接矩阵的组合，也即是将图的问题转换为矩阵的问题，虽然这是常见方法了，不过这其中对节点含义的选择，即选择文章还是作者为节点，也给我带来了一定的启发。

​    本文在分析的过程中，也给我带来了许多启发。首先是利用高相似度来进行community identification，算是比较常见的分析，不过由于存在对称和非对称的数值，分析过程值得思考和对比的地方也有很多。接下来的几部分则是让我受益颇多的地方。利用分量的组合来进行潜在相似度的分析，实际上我仅从文章分析的概念上理解到，而并未能从根本上理解其原理，即为何分量的组合能代表*i*到*j*的相似度，不过也同样扩充了自己的知识。接下来的利用质心（centroid）的分析方法，让我感觉实际上机器学习（模式识别）或是数学分析中的各类方法，其原理在各类分析中都有可能用上。在阅读了这部分之后，我认为质心分析的方法与聚类后的分析方法十分相似，相当于是以*k*方向为属性值将这些点放入空间的一个聚类，不过点之间还存在相连关系，质心也与聚类中心的地位也十分相近，不过确实，后续也提到了张量常用于多路聚类。利用质心，来代表一定量的内容来进行处理可以简化问题并找到相关内容，同时运用这样的方法也能找出远离质心却具有相同特征（本文为作者名）的内容。

​    尽管通过阅读和理解本文学到了许多知识点和思想，不过感觉自己理解的一些东西都浮于表面，还需进一步学习知识、了解原理，进行实际操作。

 

# 2 Playing Atari with Deep Reinforcement Learning

## 1.内容概述

​    本文介绍了将深度学习模型与强化学习结合，从而成功从高维视觉输入中学习到控制策略的方法，具体来说是采用卷积神经网络与Q-learning的一种变体来实现，并在Arcade学习环境中将该方法用于游玩Atari 2600游戏，结果显示在其中六个游戏上超越了以往的方法。

​    文章第一节提出了直接从高维知觉输入来学习控制策略对RL的困难，接着介绍了深度学习带来的机遇，然后分析了将二者结合的可能性与难点，最后提出了CNN+Q-learning的方法及测试环境。

​    第二节简要介绍了该方法所需要的知识，主要包括各变量表示的内容，RL的基本概念和思想，最优动作值函数Q*的定义，采用神经网络代替Q矩阵的方法。第三节介绍了与本文研究内容相关的最有名的成功方法TD-gammon和最相似的方法neural fitted Q-learning（NFQ）及一些其他将DL与RL结合的方法。

​    第四节为本文核心，介绍了本文提出的Deep Q-learning with Experience Replay原理和实现方法，特别是设置经验池，并对比TD-gammon方法分析改进之处。同时也介绍了后续实现所使用原始像素的预处理方式和模型结构。

​    第五节介绍了实验的内容和训练结果。实验内容和策略主要包括测试7个游戏，统一不同游戏的得分，采用RMSProp算法和e-greedy策略，并运用frame-skipping技术。在训练结果和值函数中对平均Q值的收敛和含义进行了简单介绍。最后对该算法进行了评估，并与Sarse和contingency算法结果进行了对比。

## 2.个人感悟与想法

​    本文相比前一篇显得更加简洁，阅读起来比较顺畅，对于其中的基本思路和想法还是比较容易理解，但对其中提及的各种方法，许多都只知其名不知其意，因此还有许多东西需要进一步学习和了解。这也让我产生了一定的疑问，如果要将一个领域下的两个方向结合，是否需要将两个方向的从诞生以来的关键性技术都从根本上进行深入学习，亦或是只需了解原理及作用，利用尝试的方法来进行探究。本文的研究思路现在看来似乎很容易想到，毕竟在前些时间DNN和CNN十分火热，且广泛用于图像处理和计算机视觉，因此将其用于代替Q-learning中的Q函数似乎也就十分合理。

​    除此之外，本文在行文过程中，通过对RL结合深度学习的难点进行分析和对相关工作的阐释，也让我学习到了一些思考问题的办法。将两个研究方向的方法结合，即将主方法中的一部分用这部分较为前沿的方法来替代是十分常用的改进方法，但是这样常常会带来难点，而这部分难点实际就是工作的重点，其解决的核心就是将新的方法与旧方法对比，通过转换或是构建新的结构来解决。同时，一项有关联的研究，了解相关工作也是十分重要的，这不仅可以为自己提供思路，也能通过与相关工作进行对比，来评估自己的方法是否高效准确。

​    本文提出的是一种十分通用的深度强化学习方法，运用同样的方法来对Atari上的多个游戏进行了学习，并取得了不错的结果。在解决问题的时候，我们常常会思考如何对一个特定问题进行，但更有效的应该是能将这一类问题的解决方法抽象为同样的办法，提出更具有普遍性的办法。本文实验结果部分，提到“即便缺少理论收敛保证，但却是达到了稳定”，就如大家常说NN可解释性差，但常常有十分不错的效果，我再次觉得许多方法其实可以先去尝试，实验和理论谁先的问题其实并不重要。

​    本文选题也很有趣，虽然这本来就是RL常见的问题，不过也让我更进一步觉得机器学习的研究，其目的是为了更好地解决生活中常见、有实用性、有趣的问题。



 

# 3  Robust Log-Based Anomaly Detection on Unstable Log Data

## 1.内容概述

​    日志被广泛用于大型软件系统的检修和维护工作，关于利用日志进行异常检测的研究也有很多。本文提出了一种新的基于日志的异常检测的方法——LogRobust，该方法利用语义向量和Bi-LSTM模型解决了在不稳定日志数据下的异常检测难题，实现了以往方法难以做到的在现实环境下的高效异常检测，并在实际测试中取得了很好的效果。

​    文章第一部分对全文内容进行了概括性的介绍。接下来在第二部分阐述了背景和实证研究，这一部分主要介绍了日志的作用和类型；日志不稳定性的实证研究，包括日志状态的更迭和处理时带来的噪声两方面；现有的基于日志异常检测方法在现实场景下的局限性。

​    第三部分对本文所提出的LogRobust方法进行了详细的解释。LogRobust共分为三步，第一步是对日志数据进行Log Parsing，将Log Message Sequence转变为Log Event；然后通过Semantic Vectorization将其转变为Semantic vector，该工作包含三个小步骤：日志事件预处理，单词向量化和基于TF-IDF聚合；最后利用attention-based Bi-LSTM神经网络来进行异常检测。该部分阐述了这三个步骤的实现方法，尤其重点介绍了后两步，最后再简单介绍了LogRobust的用法。

​    第四部分为实验部分。该部分首先介绍了实验设计，包括数据集来源、实现参数和环境、评价指标；然后回答了三个研究问题，分别为LogRobust在不稳定日志数据上的效果、运用attention mechanism在该方法上的效果、该方法在稳定日志数据上的效果。

​    文章后三部分讨论了增量更新和有效性两个问题，之后简要介绍了基于日志的异常检测和日志数据质量的相关工作，最后简单总结了本文。

## 2.个人感悟与想法

​    阅读完本文，我进一步认为即便是在一个小领域深入研究，也十分值得去了解相关领域的前沿知识以扩充自己的想法和灵感，因为对于实际性的问题，往往需要多个方面的知识才能提出创新性的解决办法。本文阅读起来很流畅，只看introduce部分似乎感觉思路十分流畅清晰，但仔细看完第三部分，只会见到比上一篇更频繁地见到各种算法和技术。首先是log parser（本文为Drain），感觉其作用就和编译原理学到的parser一样，应该也是有着十分深入和巧妙的研究；接下来是自然语言处理的简单内容；紧接着又使用了FastText算法来寻找内在联系；然后又是结合TF和IDF的聚合方法；最后是结合了attention mechanism的改进过的LSTM神经网络（Bi-LSTM），以及还有各种评价指标，方法运用可谓是十分清晰，一环扣一环。这些也进一步让我意识到团队协作和讨论的必要性，也许你了解这些步骤的其中一部分，但这部分恰好不易改进，而其他队友对另一方面颇有理解，则可在另一方面进行进一步改进，也可谓是对Amdahl定律的一种解决实际研究问题的应用吧。同时，在现有的基础方法之上引入一些有利的机制也许能带来很好的效果。

​    本文提出的方法相比于以往方法的一大改进是舍弃了传统的将日志序列转变为日志计数向量（log count vector），而是直接保留了每一个日志事件，以相同的语义向量来进行表示，从而保留了其上下文含义。在实际的数据处理中，统计出现次数（value_count）是一个十分常用的方法，用于体现某一随机事件在总事件中的出现频率，来代表某种特征。而本文没有这样思考问题，通过使用语义向量和能够处理时间序列的模型，尽管增加了一定的计算量，但是保留了日志序列之间的上下文信息，自然也能挖掘到更多信息。总的来说，虽然很多时候统计各事件出现次数是一个很好的方法，但是当出现顺序对结果有影响时，引入新的模型，考虑保留其出现顺序不乏是一种提高结果准确的方法。同时使用这种方法还能很好地适应事件类别增改，而这也是本文提出方法的一大革新之处。

​    本文的数据集构造也十分值得学习，本文共计构造了三个数据集，HDFS数据，合成后的HDFS数据和实际中的Service X数据，简单来说就是一个基准、一个人工、一个实际环境，通过注入不同比例的合成不稳定数据到基准数据，可以保证将该方法在任何一种环境进行测试，也包括现实场景，自然能更进一步地体现和确保其健壮性。

​    即使本文重点强调自己的目标是在不稳定日志上的异常检测，但是在结果部分，同样对比了与以往方法在稳定日志数据上的结果，同时本文也在一直强调兼容性（compatibility），故思考问题和解决问题的时候应该也带着这样的理念去进行，不只是为了解决一个特点的问题，而是要思考普遍的通用的方法，在现有的基础上能更进一步优化的方案。

​    三篇文章方向相近但研究内容不同，结构相似但行文风格大异，带给我许多相关概念知识、论文内容结构、问题思考策略、相关工作价值。三篇文章的整体感受为：论文思想充满魅力，方法技巧丰富多样；个人基础仍需提高，实际运用才是导向。

 

 

 